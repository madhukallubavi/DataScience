We can implement this in Keras using the LearningRateScheduler callback4 when fitting
the model. The LearningRateScheduler callback allows us to define a function to call that
takes the epoch number as an argument and returns the learning rate to use in stochastic
gradient descent. When used, the learning rate specified by stochastic gradient descent is ignored.
In the code below, we use the same example as before of a single hidden layer network on the
Ionosphere dataset. A new step decay() function is defined that implements the equation:

Where InitialLearningRate is the learning rate at the beginning of the run, EpochDrop
is how often the learning rate is dropped in epochs and DropRate is how much to drop the
learning rate each time it is dropped.

# Drop-Based Learning Rate Decay
import pandas as pd
import numpy as np
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import LearningRateScheduler

# learning rate schedule
def step_decay(epoch):
  initial_lrate = 0.1
  drop = 0.5
  epochs_drop = 10.0
  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
  return lrate

# fix random seed for reproducibility
seed = 42
np.random.seed(seed)

# load dataset
data = pd.read_csv("ionosphere.csv", header=None, skiprows=1)
array = data.values

# split into input (X) and output (Y) variables
X = array[:,0:34].astype(float)
Y = array[:,34]

# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
Y = encoder.transform(Y)

# create model
model = Sequential()
model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))

# Compile model
sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])

# learning schedule callback
lrate = LearningRateScheduler(step_decay)
callbacks_list = [lrate]

# Fit the model
model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)

Results:
Epoch 45/50
 - 0s - loss: 0.0544 - acc: 0.9830 - val_loss: 0.0753 - val_acc: 0.9914
Epoch 46/50
 - 0s - loss: 0.0536 - acc: 0.9830 - val_loss: 0.0669 - val_acc: 0.9914
Epoch 47/50
 - 0s - loss: 0.0536 - acc: 0.9830 - val_loss: 0.0661 - val_acc: 0.9914
Epoch 48/50
 - 0s - loss: 0.0530 - acc: 0.9830 - val_loss: 0.0707 - val_acc: 0.9914
Epoch 49/50
 - 0s - loss: 0.0528 - acc: 0.9830 - val_loss: 0.0713 - val_acc: 0.9914
Epoch 50/50
 - 0s - loss: 0.0525 - acc: 0.9830 - val_loss: 0.0712 - val_acc: 0.9914


Running the example results in a classication accuracy of 99.14% on the validation dataset,
again an improvement over the baseline for the model on this dataset.
