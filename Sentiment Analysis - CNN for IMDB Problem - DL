Convolutional neural networks were designed to honor the spatial structure in image data whilst
being robust to the position and orientation of learned objects in the scene. This same principle
can be used on sequences, such as the one-dimensional sequence of words in a movie review.
The same properties that make the CNN model attractive for learning to recognize objects in
images can help to learn structure in paragraphs of words, namely the techniques invariance to
the specific position of features. 

Keras supports one dimensional convolutions and pooling by the Conv1D and MaxPooling1D
classes respectively. Again, let's import the classes and functions needed for this example and
initialize our random number generator to a constant value so that we can easily reproduce
results.

#CNN for IMDB problem
import numpy as np
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers.convolutional import Conv1D, MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

#fix random seed for reproducibility
seed = 42
np.random.seed(seed)

#load dataset but only top n words, zero the rest
top_words = 5000
(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=top_words)

#pad dataset to a maximum review length in words
max_words = 500
X_train = sequence.pad_sequences(X_train, maxlen=max_words)
X_test = sequence.pad_sequences(X_test, maxlen=max_words)

#create model
model = Sequential()
model.add(Embedding(top_words, 32, input_length=max_words))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(250, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

#compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

#fit model
model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=128, verbose=2)

#final evaluation of model
scores = model.evaluate(X_test, Y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

Results:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_8 (Embedding)      (None, 500, 32)           160000    
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 500, 32)           3104      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         
_________________________________________________________________
flatten_7 (Flatten)          (None, 8000)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 250)               2000250   
_________________________________________________________________
dense_14 (Dense)             (None, 1)                 251       
=================================================================
Total params: 2,163,605
Trainable params: 2,163,605
Non-trainable params: 0
_________________________________________________________________
None
Train on 25000 samples, validate on 25000 samples
Epoch 1/2
 - 35s - loss: 0.4844 - acc: 0.7342 - val_loss: 0.2812 - val_acc: 0.8831
Epoch 2/2
 - 34s - loss: 0.2254 - acc: 0.9107 - val_loss: 0.2739 - val_acc: 0.8852
Accuracy: 88.52%

Running the example, we are first presented with a summary of the network structure.
We can see our convolutional layer preserves the dimensionality of our Embedding
input layer of 32 dimensional input with a maximum of 500 words. The pooling layer compresses
this representation by halving it. Running the example offers a small but welcome improvement
over the neural network model above with an accuracy of nearly 88.52%.
