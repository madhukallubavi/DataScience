Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given
probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras.
Dropout is only used during the training of a model and is not used when evaluating the skill of
the model. Next we will explore a few different ways of using Dropout in Keras.

The examples will use the Sonar dataset binary classification dataset. We will evaluate the 
developed models using scikit-learn with 10-fold cross-validation, in order to better tease 
out differences in the results. There are 60 input values and a single output value and the 
input values are standardized before being used in the network. The baseline neural network 
model has two hidden layers, the first with 60 units and the second with 30. Stochastic gradient
descent is used to train the model with a relatively low learning rate and momentum.

#Baseline Model on Sonar Dataset
import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import SGD
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

#fix random seed for reproducibility
seed = 42
np.random.seed(seed)

#load sonar dataset
data = pd.read_csv('sonar.all-data.csv', header=None)
array = data.values

#split into input(X) and output(Y) variables
X = array[:,0:60]
Y = array[:,60]

#encode class values as intergers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

#define baseline
def create_baseline():
  #create model
  model = Sequential()
  model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))
  model.add(Dense(30, kernel_initializer='normal', activation='relu'))
  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
  #compile model
  sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)
  model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300,
    batch_size=16, verbose=0)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

Results:
Baseline: 85.13% (4.36%)

Running the example for the baseline model without drop-out generates an estimated
classification accuracy of 85%.
