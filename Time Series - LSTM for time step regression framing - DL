We may have noticed that the data preparation for the LSTM network includes time steps.
Some sequence problems may have a varied number of time steps per sample. For example, we
may have measurements of a physical machine leading up to a point of failure or a point of
surge. Each incident would be a sample, the observations that lead up to the event would be
the time steps and the variables observed would be the features. Time steps provides another
way to phrase our time series problem. Like above in the window example, we can take prior
time steps in our time series as inputs to predict the output at the next time step.
Instead of phrasing the past observations as separate input features, we can use them as
time steps of the one input feature, which is indeed a more accurate framing of the problem.

We can do this using the same data representation as in the previous window-based example,
except when we reshape the data we set the columns to be the time steps dimension and change
the features dimension back to 1.

#LSTM for international airline passengers problem with time step regression framing
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

#convert an array of values into a dataset matrix
def create_dataset(dataset, look_back=1):
  dataX, dataY = [], []
  for i in range(len(dataset)-look_back-1):
    a = dataset[i:(i+look_back), 0]
    dataX.append(a)
    dataY.append(dataset[i + look_back, 0])
  return np.array(dataX), np.array(dataY)

#fix random seed for reproducibility
np.random.seed(42)

#load the dataset
data = pd.read_csv('international-airline-passengers.csv', usecols=[1], engine='python',
    skipfooter=3)
dataset = data.values
dataset = dataset.astype('float32')

#normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

#split into train and test sets
train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]

#reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

#reshape input to be [samples, time steps, features]
trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))
testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))

#create and fit the LSTM network
model = Sequential()
model.add(LSTM(4, input_shape=(look_back, 1)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)

#make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

#invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])

#calculate root mean squared error
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

#shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict

#shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict

#plot baseline and predictions
plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

Results:
Epoch 96/100
 - 1s - loss: 0.0029
Epoch 97/100
 - 1s - loss: 0.0029
Epoch 98/100
 - 1s - loss: 0.0028
Epoch 99/100
 - 1s - loss: 0.0028
Epoch 100/100
 - 1s - loss: 0.0027
Train Score: 27.18 RMSE
Test Score: 66.25 RMSE

We can see that the results are slightly better than previous example, and the structure of
the input data makes a lot more sense.
