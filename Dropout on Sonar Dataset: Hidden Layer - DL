Dropout can be applied to hidden neurons in the body of our network model. In the example
below dropout is applied between the two hidden layers andx between the last hidden layer and
the output layer. Again a dropout rate of 20% is used as is a weight constraint on those layers.

#Dropout on Sonar Dataset: Hidden Layer
import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.layers import Dropout
from keras.constraints import maxnorm
from keras.optimizers import SGD
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

#fix random seed for reproducibility
seed = 42
np.random.seed(seed)

#load sonar dataset
data = pd.read_csv('sonar.all-data.csv', header=None)
array = data.values

#split into input(X) and output(Y) variables
X = array[:,0:60].astype(float)
Y = array[:,60]

#encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

#dropout in hidden layers with weight constraint
def create_model():
  #create model
  model = Sequential()
  model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu',
  kernel_constraint=maxnorm(3)))
  model.add(Dropout(0.2))
  model.add(Dense(30, kernel_initializer='normal', activation='relu',
       kernel_constraint=maxnorm(3)))
  model.add(Dropout(0.2))
  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
  #compile model
  sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)
  model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
  return model

np.random.seed(seed)
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300,
  batch_size=16, verbose=0)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print("Hidden: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

Results:
Hidden: 86.56% (4.08%)

We can see that for this problem and for the chosen network configuration that using dropout
in the hidden layers did not lift performance. In fact, performance was worse than the baseline.
It is possible that additional training epochs are required or that further tuning is required to
the learning rate.
