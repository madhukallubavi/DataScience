We can see dropout having the desired impact on training with a slightly slower trend
in convergence and in this case a lower final accuracy. The model could probably use a few
more epochs of training and may achieve a higher skill. Alternately, dropout can be applied 
to the input and recurrent connections of the memory units with the LSTM precisely and separately.
Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the 
input dropout and recurrent dropout for configuring the recurrent dropout. For example, we can
modify the first example to add dropout to the input and recurrent connections as follows:

#LSTM with recurrent dropout for sequence classification in the IMDB dataset
import numpy as np
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

#fix random seed for reproducibility
np.random.seed(42)

#load the dataset but only keep the top n words, zero the rest
top_words = 5000
(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=top_words)

#truncate and pad input sequences
max_review_length = 500
X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)

#create the model
embedding_vecor_length = 32
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, 

input_length=max_review_length))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, Y_train, epochs=3, batch_size=64)

#Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

Results:
Epoch 1/3
25000/25000 [==============================] - 450s 18ms/step - loss: 0.4989 - 
acc: 0.7569
Epoch 2/3
25000/25000 [==============================] - 442s 18ms/step - loss: 0.3601 - 
acc: 0.8486
Epoch 3/3
25000/25000 [==============================] - 444s 18ms/step - loss: 0.4158 - 
acc: 0.8197
Accuracy: 82.62%

We can see that the LSTM specific dropout has a more pronounced effect on the convergence
of the network than the layer-wise dropout. As above, the number of epochs was kept constant
and could be increased to see if the skill of the model can be further lifted. Dropout is a powerful
technique for combating overfitting in your LSTM models and it is a good idea to try both
methods, but we may bet better results with the gate-specific dropout provided in Keras.
