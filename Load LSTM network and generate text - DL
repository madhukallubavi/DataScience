Generating text using the trained LSTM network is relatively straightforward. Firstly, we load
the data and define the network in exactly the same way, except the network weights are loaded
from a checkpoint file and the network does not need to be trained.

Finally, we need to actually make predictions. The simplest way to use the Keras LSTM
model to make predictions is to first start of with a seed sequence as input, generate the next
character then update the seed sequence to add the generated character on the end and trim of
the first character. This process is repeated for as long as we want to predict new characters
(e.g. a sequence of 1,000 characters in length). We can pick a random input pattern as our seed
sequence, then print generated characters as we generate them.

#Load LSTM network and generate text
import sys
import numpy as np,h5py 
import os.path
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.utils import np_utils

#load ASCII text and covert to lowercase
filename = "alice_in_wonderland.txt"
raw_text = open(filename).read()
raw_text = raw_text.lower()

#create mapping of unique chars to integers, and a reverse mapping
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

#summarize the loaded data
n_chars = len(raw_text)
n_vocab = len(chars)
print("Total Characters: ", n_chars)
print("Total Vocab: ", n_vocab)

#prepare the dataset of input to output pairs encoded as integers
seq_length = 100
dataX = []
dataY = []
for i in range(0, n_chars - seq_length, 1):
  seq_in = raw_text[i:i + seq_length]
  seq_out = raw_text[i + seq_length]
  dataX.append([char_to_int[char] for char in seq_in])
  dataY.append(char_to_int[seq_out])
n_patterns = len(dataX)
print("Total Patterns: ", n_patterns)

#reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (n_patterns, seq_length, 1))

#normalize
X = X / float(n_vocab)

#one hot encode the output variable
Y = np_utils.to_categorical(dataY)

#define the LSTM model
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))
model.add(Dense(Y.shape[1], activation='softmax'))

#load the network weights
filename = "weights-improvement-20-2.2967.hdf5"  
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')

#pick a random seed
start = np.random.randint(0, len(dataX)-1)
pattern = dataX[start]
print("Seed:")
print("\"", ''.join([int_to_char[value] for value in pattern]), "\"")

#generate characters
for i in range(1000):
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x / float(n_vocab)
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  seq_in = [int_to_char[value] for value in pattern]
  sys.stdout.write(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]
print("\nDone.")

Results:
Total Characters:  148574
Total Vocab:  46
Total Patterns:  148474
Seed:
" u're sure to do that,' said the cat, `if you only walk
long enough.'

  alice felt that this could n "
o the woo oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the woe oo the 
woe oo the woe 
Done.

Running this example first outputs the selected random seed, then each character as it is
generated.

We can note some observations about the generated text.
 It generally conforms to the line format observed in the original text of less than 80
characters before a new line.
 The characters are separated into word-like groups and most groups are actual English
words (e.g. the), but many do not (e.g. oo, woe).

The fact that this character based model of the book produces output like this is very
impressive. It gives you a sense of the learning capabilities of LSTM networks. The results are
not perfect.
