An increase in the number of filters used in the convolutional layer can often improve performance,
as it can provide more opportunity for extracting simple features from the input images. This is
especially relevant when very small filters are used, such as 3 × 3 pixels. In this change, we can
increase the number of filters in the convolutional layer from 32 to double that at 64. We will
also build upon the possible improvement offered by using ‘same’ padding.

#Model with double the filters for fashion MNIST
from numpy import mean, std
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from keras.optimizers import SGD

#load train and test dataset
def load_dataset():
  #load dataset
  (trainX, trainY), (testX, testY) = fashion_mnist.load_data()
  #reshape dataset to have a single channel
  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
  testX = testX.reshape((testX.shape[0], 28, 28, 1))
  #one hot encode target values
  trainY = to_categorical(trainY)
  testY = to_categorical(testY)
  return trainX, trainY, testX, testY

#scale pixels
def prep_pixels(train, test):
  #convert from integers to floats
  train_norm = train.astype('float32')
  test_norm = test.astype('float32')
  #normalize to range 0-1
  train_norm = train_norm / 255.0
  test_norm = test_norm / 255.0
  #return normalized images
  return train_norm, test_norm

#define cnn model
def define_model():
  model = Sequential()
  model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
  model.add(MaxPooling2D((2, 2)))
  model.add(Flatten())
  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
  model.add(Dense(10, activation='softmax'))
  #compile model
  opt = SGD(lr=0.01, momentum=0.9)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

#evaluate a model using k-fold cross-validation
def evaluate_model(model, dataX, dataY, n_folds=5):
  scores, histories = list(), list()
  #prepare cross validation
  kfold = KFold(n_folds, shuffle=True, random_state=1)
  #enumerate splits
  for train_ix, test_ix in kfold.split(dataX):
    #select rows for train and test
    trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
    #fit model
    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
    #evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    #append scores
    scores.append(acc)
    histories.append(history)
  return scores, histories

#plot diagnostic learning curves
def summarize_diagnostics(histories):
  for i in range(len(histories)):
    #plot loss
    plt.subplot(211)
    plt.title('Cross Entropy Loss')
    plt.plot(histories[i].history['loss'], color='blue', label='train')
    plt.plot(histories[i].history['val_loss'], color='orange', label='test')
    #plot accuracy
    plt.subplot(212)
    plt.title('Classification Accuracy')
    plt.plot(histories[i].history['acc'], color='blue', label='train')
    plt.plot(histories[i].history['val_acc'], color='orange', label='test')
  plt.show()

#summarize model performance
def summarize_performance(scores):
  #print summary
  print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
  #box and whisker plots of results
  plt.boxplot(scores)
  plt.show()

#run the test harness for evaluating a model
def run_test_harness():
  #load dataset
  trainX, trainY, testX, testY = load_dataset()
  #prepare pixel data
  trainX, testX = prep_pixels(trainX, testX)
  #define model
  model = define_model()
  #evaluate model
  scores, histories = evaluate_model(model, trainX, trainY)
  #learning curves
  summarize_diagnostics(histories)
  #summarize estimated performance
  summarize_performance(scores)

#entry point, run the test harness
run_test_harness()

Results:
> 91.225
> 95.242
> 97.950
> 99.500
> 99.958
Accuracy: mean=96.775 std=3.228, n=5

Running the example reports model performance for each fold of the cross-validation process.

Note: Your specific results may vary given the stochastic nature of the learning algorithm.
Consider running the example a few times and compare the average performance.

Next, the estimated performance of the model is presented, showing a small improvement in
performance as compared to the baseline with padding from 96.775% to 95.863%. Again, the
change is still within the bounds of the standard deviation, and it is not clear whether the effect
is real.

In this case, the per-fold scores may suggest some further improvement over the baseline
and using same padding alone.

A plot of the learning curves is created, in this case showing that the models still have a
reasonable fit on the problem, with a small sign of some of the runs overfitting.
