The Keras implementation of LSTMs resets the state of the network after each batch. This
suggests that if we had a batch size large enough to hold all input patterns and if all the input
patterns were ordered sequentially, that the LSTM could use the context of the sequence within
the batch to better learn the sequence. We can demonstrate this easily by modifying the first
example for learning a one-to-one mapping and increasing the batch size from 1 to the size
of the training dataset. Additionally, Keras shuffles the training dataset before each training
epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.

The network will learn the mapping of characters using the within-batch sequence, but this
context will not be available to the network when making predictions. We can evaluate both
the ability of the network to make predictions randomly and in sequence.

#Naive LSTM to learn one-char to one-char mapping with all data in each batch
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.utils import np_utils
from keras.preprocessing.sequence import pad_sequences

#fix random seed for reproducibility
np.random.seed(42)

#define the raw dataset
alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

#create mapping of characters to integers (0-25) and the reverse
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))

#prepare the dataset of input to output pairs encoded as integers
seq_length = 1
dataX = []
dataY = []
for i in range(0, len(alphabet) - seq_length, 1):
  seq_in = alphabet[i:i + seq_length]
  seq_out = alphabet[i + seq_length]
  dataX.append([char_to_int[char] for char in seq_in])
  dataY.append(char_to_int[seq_out])
  print(seq_in, '->', seq_out)

#convert list of lists to array and pad sequences if needed
X = pad_sequences(dataX, maxlen=seq_length, dtype='float32')

#reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (X.shape[0], seq_length, 1))

#normalize
X = X / float(len(alphabet))

#one hot encode the output variable
Y = np_utils.to_categorical(dataY)

#create and fit the model
model = Sequential()
model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))
model.add(Dense(Y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=5000, batch_size=len(dataX), verbose=2, shuffle=False)

#summarize performance of the model
scores = model.evaluate(X, Y, verbose=0)
print("Model Accuracy: %.2f%%" % (scores[1]*100))

#demonstrate some model predictions
for pattern in dataX:
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x / float(len(alphabet))
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  seq_in = [int_to_char[value] for value in pattern]
  print(seq_in, "->", result)

#demonstrate predicting random patterns
print("Test a Random Pattern:")
for i in range(0,20):
  pattern_index = np.random.randint(len(dataX))
  pattern = dataX[pattern_index]
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x / float(len(alphabet))
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  seq_in = [int_to_char[value] for value in pattern]
  print(seq_in, "->", result)

Results:
Model Accuracy: 100.00%
['A'] -> B
['B'] -> C
['C'] -> D
['D'] -> E
['E'] -> F
['F'] -> G
['G'] -> H
['H'] -> I
['I'] -> J
['J'] -> K
['K'] -> L
['L'] -> M
['M'] -> N
['N'] -> O
['O'] -> P
['P'] -> Q
['Q'] -> R
['R'] -> S
['S'] -> T
['T'] -> U
['U'] -> V
['V'] -> W
['W'] -> X
['X'] -> Y
['Y'] -> Z
Test a Random Pattern:
['R'] -> S
['T'] -> U
['O'] -> P
['G'] -> H
['K'] -> L
['T'] -> U
['P'] -> Q
['D'] -> E
['F'] -> G
['V'] -> W
['C'] -> D
['I'] -> J
['J'] -> K
['N'] -> O
['K'] -> L
['R'] -> S
['V'] -> W
['L'] -> M
['W'] -> X
['L'] -> M

As we expected, the network is able to use the within-sequence context to learn the alphabet,
achieving 100% accuracy on the training data. Importantly, the network can make accurate
predictions for the next letter in the alphabet for randomly selected characters, very impressive.
