Let's start of by designing a simple LSTM to learn how to predict the next character in the
alphabet given the context of just one character. We will frame the problem as a random
collection of one-letter input to one-letter output pairs. As we will see this is a difficult framing of
the problem for the LSTM to learn. Let's define an LSTM network with 32 units and an output
layer using the softmax activation function for making predictions. Because this is a multiclass
classification problem, we can use the log loss function (called categorical crossentropy in
Keras), and optimize the network using the ADAM optimization function. The model is fit over
500 epochs with a batch size of 1.

#Naive LSTM to learn one-char to one-char mapping
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.utils import np_utils

#fix random seed for reproducibility
np.random.seed(47)

#define raw dataset
alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

#create mapping of characters to integers (0-25) and the reverse
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))

#prepare dataset of input to output pairs encoded as integers
seq_length = 1
dataX=[]
dataY=[]
for i in range(0, len(alphabet) - seq_length, 1):
  seq_in = alphabet[i:i+seq_length]
  seq_out = alphabet[i + seq_length]
  dataX.append([char_to_int[char] for char in seq_in])
  dataY.append(char_to_int[seq_out])
  print(seq_in, '->', seq_out)

#reshape X to be [samples, time steps, features]
X = np.reshape(dataX, (len(dataX), seq_length, 1))

#normalize
X=X/float(len(alphabet))

#one hot encode output variable
Y = np_utils.to_categorical(dataY)

#create and fit model
model = Sequential()
model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))
model.add(Dense(Y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=500, batch_size=1, verbose=2)

#summarize performance of model
scores = model.evaluate(X, Y, verbose=0)
print("Model Accuracy: %.2f%%" % (scores[1]*100))

#demonstrate some model predictions
for pattern in dataX:
  x = np.reshape(pattern, (1, len(pattern), 1))
  x = x/float(len(alphabet))
  prediction = model.predict(x, verbose=0)
  index = np.argmax(prediction)
  result = int_to_char[index]
  seq_in = [int_to_char[value] for value in pattern]
  print(seq_in, '->', result)

Results:
Model Accuracy: 84.00%
['A'] -> B
['B'] -> B
['C'] -> D
['D'] -> E
['E'] -> F
['F'] -> G
['G'] -> H
['H'] -> I
['I'] -> J
['J'] -> K
['K'] -> L
['L'] -> M
['M'] -> N
['N'] -> O
['O'] -> P
['P'] -> Q
['Q'] -> R
['R'] -> S
['S'] -> T
['T'] -> U
['U'] -> V
['V'] -> X
['W'] -> Z
['X'] -> Z
['Y'] -> Z

We can see that this problem is indeed difficult for the network to learn. The reason is, the
poor LSTM units do not have any context to work with. Each input-output pattern is shown to
the network in a random order and the state of the network is reset after each pattern (each
batch where each batch contains one pattern). This is abuse of the LSTM network architecture,
treating it like a standard Multilayer Perceptron.
