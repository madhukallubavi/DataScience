In the previous two sections, we discovered that both dropout and data augmentation resulted
in a significant improvement in model performance. In this section, we can experiment with
combining both of these changes to the model to see if a further improvement can be achieved.
Specifically, whether using both regularization techniques together results in better performance
than either technique used alone. The full code listing of a model with fixed dropout and data
augmentation is provided below for completeness.

#Baseline: Dropout and Data Augmentation on CIFAR10 Dataset
import sys
from matplotlib import pyplot as plt
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dropout

#load train and test dataset
def load_dataset():
  #load dataset
  (trainX, trainY), (testX, testY) = cifar10.load_data()
  #one hot encode target values
  trainY = to_categorical(trainY)
  testY = to_categorical(testY)
  return trainX, trainY, testX, testY

#scale pixels
def prep_pixels(train, test):
  #convert from integers to floats
  train_norm = train.astype('float32')
  test_norm = test.astype('float32')
  #normalize to range 0-1
  train_norm = train_norm / 255.0
  test_norm = test_norm / 255.0
  #return normalized images
  return train_norm, test_norm

#define cnn model
def define_model():
  model = Sequential()
  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same', input_shape=(32, 32, 3)))
  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Flatten())
  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
  model.add(Dropout(0.2))
  model.add(Dense(10, activation='softmax'))
  #compile model
  opt = SGD(lr=0.001, momentum=0.9)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

#plot diagnostic learning curves
def summarize_diagnostics(history):
  #plot loss
  plt.subplot(211)
  plt.title('Cross Entropy Loss')
  plt.plot(history.history['loss'], color='blue', label='train')
  plt.plot(history.history['val_loss'], color='orange', label='test')
  #plot accuracy
  plt.subplot(212)
  plt.title('Classification Accuracy')
  plt.plot(history.history['acc'], color='blue', label='train')
  plt.plot(history.history['val_acc'], color='orange', label='test')
  #save plot to file
  filename = sys.argv[0].split('/')[-1]
  plt.savefig(filename + '_plot.png')
  plt.close()

#run the test harness for evaluating a model
def run_test_harness():
  #load dataset
  trainX, trainY, testX, testY = load_dataset()
  #prepare pixel data
  trainX, testX = prep_pixels(trainX, testX)
  #define model
  model = define_model()
  #create data generator
  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,
                               horizontal_flip=True)
  #prepare iterator
  it_train = datagen.flow(trainX, trainY, batch_size=64)
  #fit model
  steps = int(trainX.shape[0] / 64)
  history = model.fit_generator(it_train, steps_per_epoch=steps, epochs=200,
                                validation_data=(testX, testY), verbose=0)
  #evaluate model
  _, acc = model.evaluate(testX, testY, verbose=0)
  print('> %.3f' % (acc * 100.0))
  #learning curves
  summarize_diagnostics(history)

#entry point, run the test harness
run_test_harness()

Results:
> 86.570

Running the model in the test harness prints the classification accuracy on the test dataset.

Note: Your specific results may vary given the stochastic nature of the learning algorithm.
Consider running the example a few times and compare the average performance.

In this case, we can see that as we would have hoped, using both regularization techniques
together has resulted in a further lift in model performance on the test set. In this case,
combining fixed dropout with about 83% and data augmentation with about approximately 84% has resulted
in an improvement to about 86% classification accuracy.

Reviewing the learning curves, we can see that the convergence behavior of the model is
also better than either fixed dropout and data augmentation alone. Learning has been slowed
without overfitting, allowing continued improvement. The plot also suggests that learning may
not have stalled and may have continued to improve if allowed to continue, but perhaps very
modestly. Results might be further improved if a pattern of increasing dropout was used instead
of a fixed dropout rate throughout the depth of the model.
