Dropout is a simple technique that will randomly drop nodes out of the network. It has a
regularizing effect as the remaining nodes must adapt to pick-up the slack of the removed nodes.
Dropout can be added to the model by adding new Dropout layers, where the amount of nodes
removed is specified as a parameter. There are many patterns for adding Dropout to a model,
in terms of where in the model to add the layers and how much dropout to use. In this case, we
will add Dropout layers after each max pooling layer and after the fully connected layer, and
use a fixed dropout rate of 20% (e.g. retain 80% of the nodes). The updated VGG 3 baseline
model with dropout is listed below.

#Baseline: Dropout Regularization with CIFAR10 dataset
import sys
from matplotlib import pyplot as plt
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from keras.optimizers import SGD

#load train and test dataset
def load_dataset():
  #load dataset
  (trainX, trainY), (testX, testY) = cifar10.load_data()
  #one hot encode target values
  trainY = to_categorical(trainY)
  testY = to_categorical(testY)
  return trainX, trainY, testX, testY

#scale pixels
def prep_pixels(train, test):
  #convert from integers to floats
  train_norm = train.astype('float32')
  test_norm = test.astype('float32')
  #normalize to range 0-1
  train_norm = train_norm / 255.0
  test_norm = test_norm / 255.0
  #return normalized images
  return train_norm, test_norm

#define cnn model
def define_model():
  model = Sequential()
  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same', input_shape=(32, 32, 3)))
  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',
                   padding='same'))
  model.add(MaxPooling2D((2, 2)))
  model.add(Dropout(0.2))
  model.add(Flatten())
  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
  model.add(Dropout(0.2))
  model.add(Dense(10, activation='softmax'))
  #compile model
  opt = SGD(lr=0.001, momentum=0.9)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

#plot diagnostic learning curves
def summarize_diagnostics(history):
  #plot loss
  plt.subplot(211)
  plt.title('Cross Entropy Loss')
  plt.plot(history.history['loss'], color='blue', label='train')
  plt.plot(history.history['val_loss'], color='orange', label='test')
  #plot accuracy
  plt.subplot(212)
  plt.title('Classification Accuracy')
  plt.plot(history.history['acc'], color='blue', label='train')
  plt.plot(history.history['val_acc'], color='orange', label='test')
  #save plot to file
  filename = sys.argv[0].split('/')[-1]
  plt.savefig(filename + '_plot.png')
  plt.close()

#run the test harness for evaluating a model
def run_test_harness():
  #load dataset
  trainX, trainY, testX, testY = load_dataset()
  #prepare pixel data
  trainX, testX = prep_pixels(trainX, testX)
  #define model
  model = define_model()
  #fit model
  history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
  #evaluate model
  _, acc = model.evaluate(testX, testY, verbose=0)
  print('> %.3f' % (acc * 100.0))
  #learning curves
  summarize_diagnostics(history)

#entry point, run the test harness
run_test_harness()

Results:
> 83.300

Running the model in the test harness prints the classification accuracy on the test dataset.

Note: Your specific results may vary given the stochastic nature of the learning algorithm.
Consider running the example a few times and compare the average performance.

In this case, we can see a jump in classification accuracy by about 9% from about 74%
without dropout to about 83% with dropout.

Reviewing the learning curve for the model, we can see that overfitting has been reduced. The
model converges well for about 40 or 50 epochs, at which point there is no further improvement
on the test dataset. This is a great result. We could elaborate upon this model and add early
stopping with a patience of about 10 epochs to save a well-performing model on the test set
during training at around the point that no further improvements are observed. We could also
try exploring a learning rate schedule that drops the learning rate after improvements on the
test set stall. Dropout has performed well, and we do not know that the chosen rate of 20% is
the best. We could explore other dropout rates, as well as different positioning of the dropout
layers in the model architecture.
