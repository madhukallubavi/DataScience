A key innovation in the ResNet was the residual module. The residual module, specifically the
identity residual model, is a block of two convolutional layers with the same number of filters
and a small filter size where the output of the second layer is added with the input to the first
convolutional layer. Drawn as a graph, the input to the module is added to the output of the
module and is called a shortcut connection. We can implement this directly in Keras using the
functional API and the add() merge function.

A limitation with this direct implementation is that if the number of filters in the input
layer does not match the number of filters in the last convolutional layer of the module (defined
by n filters), then we will get an error. One solution is to use a 1 Ã— 1 convolution layer,
often referred to as a projection layer, to either increase the number of filters for the input
layer or reduce the number of filters for the last convolutional layer in the module. The former
solution makes more sense, and is the approach proposed in the paper, referred to as a projection
shortcut.

#CNN model with identity or projection residual module
from keras.models import Model
from keras.layers import Input, Activation, Conv2D, add
from keras.utils import plot_model

#function for creating an identity or projection residual module
def residual_module(layer_in, n_filters):
  merge_input = layer_in
  #check if the number of filters needs to be increase, assumes channels last format
  if layer_in.shape[-1] != n_filters:
    merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu',
        kernel_initializer='he_normal')(layer_in)
  #conv1
  conv1 = Conv2D(n_filters, (3,3), padding='same', activation='relu',
        kernel_initializer='he_normal')(layer_in)
  #conv2
  conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear',
        kernel_initializer='he_normal')(conv1)
  #add filters, assumes filters/channels last
  layer_out = add([conv2, merge_input])
  #activation function
  layer_out = Activation('relu')(layer_out)
  return layer_out

#define model input
visible = Input(shape=(256, 256, 3))
#add vgg module
layer = residual_module(visible, 64)

#create model
model = Model(inputs=visible, outputs=layer)

#summarize model
model.summary()

#plot model architecture
plot_model(model, show_shapes=True, to_file='residual_module.png')

Results:
W0625 13:29:01.849419 140459062425472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 256, 256, 3)  0                                            
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 256, 256, 64) 1792        input_11[0][0]                   
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_39[0][0]                  
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 256, 256, 64) 256         input_11[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_40[0][0]                  
                                                                 conv2d_38[0][0]                  
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 256, 256, 64) 0           add_1[0][0]                      
==================================================================================================
Total params: 38,976
Trainable params: 38,976
Non-trainable params: 0
__________________________________________________________________________________________________

Running the example first creates the model then prints a summary of the layers. Because
the module is linear, this summary is helpful to see what is going on.
