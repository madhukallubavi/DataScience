There are many ways that we might explore improvements to the baseline model. We will look
at areas that often result in an improvement, so-called low-hanging fruit. The first will be
a change to the convolutional operation to add padding and the second will build on this to
increase the number of filters.

Adding padding to the convolutional operation can often result in better model performance, as
more of the input image of feature maps are given an opportunity to participate or contribute
to the output. By default, the convolutional operation uses ‘valid’ padding, which means that
convolutions are only applied where possible. This can be changed to ‘same’ padding so that
zero values are added around the input such that the output has the same size as the input.

#Model with padded convolutions for fashion MNIST
from numpy import mean, std
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from keras.optimizers import SGD

#load train and test dataset
def load_dataset():
  #load dataset
  (trainX, trainY), (testX, testY) = fashion_mnist.load_data()
  #reshape dataset to have a single channel
  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
  testX = testX.reshape((testX.shape[0], 28, 28, 1))
  #one hot encode target values
  trainY = to_categorical(trainY)
  testY = to_categorical(testY)
  return trainX, trainY, testX, testY

#scale pixels
def prep_pixels(train, test):
  #convert from integers to floats
  train_norm = train.astype('float32')
  test_norm = test.astype('float32')
  #normalize to range 0-1
  train_norm = train_norm / 255.0
  test_norm = test_norm / 255.0
  #return normalized images
  return train_norm, test_norm

#define cnn model
def define_model():
  model = Sequential()
  model.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
  model.add(MaxPooling2D((2, 2)))
  model.add(Flatten())
  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
  model.add(Dense(10, activation='softmax'))
  #compile model
  opt = SGD(lr=0.01, momentum=0.9)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

#evaluate a model using k-fold cross-validation
def evaluate_model(model, dataX, dataY, n_folds=5):
  scores, histories = list(), list()
  #prepare cross validation
  kfold = KFold(n_folds, shuffle=True, random_state=1)
  #enumerate splits
  for train_ix, test_ix in kfold.split(dataX):
    #select rows for train and test
    trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
    #fit model
    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
    #evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    #append scores
    scores.append(acc)
    histories.append(history)
  return scores, histories

#plot diagnostic learning curves
def summarize_diagnostics(histories):
  for i in range(len(histories)):
    #plot loss
    plt.subplot(211)
    plt.title('Cross Entropy Loss')
    plt.plot(histories[i].history['loss'], color='blue', label='train')
    plt.plot(histories[i].history['val_loss'], color='orange', label='test')
    #plot accuracy
    plt.subplot(212)
    plt.title('Classification Accuracy')
    plt.plot(histories[i].history['acc'], color='blue', label='train')
    plt.plot(histories[i].history['val_acc'], color='orange', label='test')
  plt.show()

#summarize model performance
def summarize_performance(scores):
  #print summary
  print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
  #box and whisker plots of results
  plt.boxplot(scores)
  plt.show()

#run the test harness for evaluating a model
def run_test_harness():
  #load dataset
  trainX, trainY, testX, testY = load_dataset()
  #prepare pixel data
  trainX, testX = prep_pixels(trainX, testX)
  #define model
  model = define_model()
  #evaluate model
  scores, histories = evaluate_model(model, trainX, trainY)
  #learning curves
  summarize_diagnostics(histories)
  #summarize estimated performance
  summarize_performance(scores)

#entry point, run the test harness
run_test_harness()

Results:
> 90.050
> 93.675
> 96.917
> 98.817
> 99.858
Accuracy: mean=95.863 std=3.588, n=5

Running the example again reports model performance for each fold of the cross-validation
process.

Note: Your specific results may vary given the stochastic nature of the learning algorithm.
Consider running the example a few times and compare the average performance.

In this case, we can see perhaps a small improvement in model performance as compared to
the baseline across the cross-validation folds.

A plot of the learning curves is created. As with the baseline model, we may see some slight
overfitting. This could be addressed perhaps with use of regularization or the training for fewer
epochs.

Next, the estimated performance of the model is presented, showing performance with a
slight increase in the mean accuracy of the model, 95.863% as compared to 96.968% with the
baseline model. This may or may not be a real effect as it is within the bounds of the standard
deviation. Perhaps more repeats of the experiment could tease out this fact.
